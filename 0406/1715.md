## Head first machine learning 
### Gianmarco Forcella

### Introduction to AI
There is no shared definition. PoliMi says: it's a branch of CS which studies
the development of HW and SW systems equipped with abilities similar to human
ones. 

We will build a simple ML system that determines whether a Pokemon is legendary
or not.

For ML, we need a **dataset** = collection of data. Slightly different from a
database: datasets can be presented in many different formats -- db, Excel, tsv,
json, usually csv. There are infinite ways in which a dataset can be presented,
because they can represent many different things and come from many different
organizations.

### Step 1: finding data
It's usually the longest step (sometimes you need to scrape the web, it's really
hard to be given it).

### Step 2: loading data
Loading it is usually easy (`pd.read_csv`). 

### Step 3: data preprocessing
Sometimes there is feature engineering to do (feature selection, identification,
extraction). In our case we have a ready-to-use dataset -- but we'd better
always use the correlation matrix to see correlation between variables, because
in order to use a dataset it must be statistically good. Python libraries we
might use: `pandas` (basic approach) or `koalas` (for a distributed approach).

**Pandas** is the most used library to read datasets and do some statistics on
it. 
**Correlation matrix** is done with `data.corr()` --> a good correlation is
between 0.3 and 0.7. It prints a  coloured matrix and the darker it is, the
worse. If data is too correlated, it's better NOT to use the dataset and use a
different one.

### Step 3.1: erasing data
In this case, we'll drop the Pokemon name and types that are two
_columns_ which is the same as _features_. 

### Step 3.2: check and handle missing data
Check if we have any `NaN` values --> how should we treat them? There are two
ways to treat them: drop the entire column, or you insert the average for the
entire column. This second way might be very risky.

### Step 4: choosing the model
- Supervised: you give labels to the algorithm and classify the data
- Unsupervised = clustering
- Reinforcement 

Most frequently used models in ML: decision tree / neural network.

Libraries: "blackbox" ML toolkits such as Scikit Learn, deep learning models
such as PyTorch, TensorFlow, ...

### Step 5: training a model
You create a separated test dataset and train the model --> fit it.

### Step 6: getting the metrics out of the model
In our example, decision tree got 92% accuracy, MLP (=?) 88% --> they both are
good. 

### Step 7 (not necessary): refining the model
It's impossible to get 100% accuracy. However, you can do **hyperparameters
tuning**. 

### Take-home messages
Scikit Learn is the "playground" for simple things; if you want to do something
a bit more complex use the deep learning frameworks.

---

### Q&A
Limit of Pandas: with large datasets. Koalas is based on the Spark ecosystem (?)
--> better for big data because it has a distributed approach --> faster.

If you have a strong correlation between two features, the ML system cannot
distinguish between these two features --> decide about which column it is
related. 